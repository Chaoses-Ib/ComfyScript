{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux Examples\n",
    "From https://comfyanonymous.github.io/ComfyUI_examples/flux/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from comfy_script.runtime import *\n",
    "\n",
    "# load('http://127.0.0.1:8188/')\n",
    "load()\n",
    "\n",
    "# Nodes can only be imported after load()\n",
    "from comfy_script.runtime.nodes import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Workflow():\n",
    "    model, clip, vae = CheckpointLoaderSimple('flux1-dev-fp8.safetensors')\n",
    "\n",
    "    positive_conditioning = CLIPTextEncode('cute anime girl with massive fluffy fennec ears and a big fluffy tail blonde messy long hair blue eyes wearing a maid outfit with a long black gold leaf pattern dress and a white apron mouth open placing a fancy black forest cake with candles on top of a dinner table of an old dark Victorian mansion lit by candlelight with a bright window to the foggy forest and very expensive stuff everywhere there are paintings on the walls', clip)\n",
    "    positive_conditioning = FluxGuidance(positive_conditioning, 3.5)\n",
    "    negative_conditioning = CLIPTextEncode('', clip)\n",
    "\n",
    "    latent = EmptySD3LatentImage(1024, 1024, 1)\n",
    "    # Note that Flux dev and schnell do not have any negative prompt so CFG should be set to 1.0. Setting CFG to 1.0 means the negative prompt is ignored.\n",
    "    latent = KSampler(model, 972054013131368, 20, 1, 'euler', 'simple', positive_conditioning, negative_conditioning, latent, 1)\n",
    "    image = VAEDecode(latent, vae)\n",
    "    SaveImage(image, 'ComfyUI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP8 Checkpoint version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Workflow():\n",
    "    '''If you get an error in any of the nodes above make sure the files are in the correct directories.\n",
    "\n",
    "    See the top of the examples page for the links : https://comfyanonymous.github.io/ComfyUI_examples/flux/\n",
    "\n",
    "    flux1-dev.safetensors goes in: ComfyUI/models/unet/\n",
    "\n",
    "    t5xxl_fp16.safetensors and clip_l.safetensors go in: ComfyUI/models/clip/\n",
    "\n",
    "    ae.safetensors goes in: ComfyUI/models/vae/\n",
    "\n",
    "\n",
    "    Tip: You can set the weight_dtype above to one of the fp8 types if you have memory issues.'''\n",
    "\n",
    "    noise = RandomNoise(219670278747233)\n",
    "\n",
    "    width = 1024\n",
    "    height = 1024\n",
    "\n",
    "    model = UNETLoader('flux1-dev.safetensors', 'default')\n",
    "    # The reference sampling implementation auto adjusts the shift value based on the resolution, if you don't want this you can just comment out (Ctrl+/) this ModelSamplingFlux node.\n",
    "    model = ModelSamplingFlux(model, 1.15, 0.5, width, height)\n",
    "\n",
    "    clip = DualCLIPLoader('t5xxl_fp16.safetensors', 'clip_l.safetensors', 'flux')\n",
    "    positive_conditioning = CLIPTextEncode('cute anime girl with massive fluffy fennec ears and a big fluffy tail blonde messy long hair blue eyes wearing a maid outfit with a long black gold leaf pattern dress and a white apron mouth open holding a fancy black forest cake with candles on top in the kitchen of an old dark Victorian mansion lit by candlelight with a bright window to the foggy forest and very expensive stuff everywhere', clip)\n",
    "    positive_conditioning = FluxGuidance(positive_conditioning, 3.5)\n",
    "\n",
    "    guider = BasicGuider(model, positive_conditioning)\n",
    "    sampler = KSamplerSelect('euler')\n",
    "    sigmas = BasicScheduler(model, 'simple', 20, 1)\n",
    "    latent = EmptySD3LatentImage(width, height, 1)\n",
    "    latent, _ = SamplerCustomAdvanced(noise, guider, sampler, sigmas, latent)\n",
    "    vae = VAELoader('ae.safetensors')\n",
    "    image = VAEDecode(latent, vae)\n",
    "    SaveImage(image, 'ComfyUI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redux\n",
    "The redux model lets you prompt with images. It can be used with any Flux1 dev or schnell model workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Workflow():\n",
    "    '''If you get an error in any of the nodes above make sure the files are in the correct directories.\n",
    "\n",
    "    See the top of the examples page for the links : https://comfyanonymous.github.io/ComfyUI_examples/flux/\n",
    "\n",
    "    flux1-dev.safetensors goes in: ComfyUI/models/unet/\n",
    "\n",
    "    t5xxl_fp16.safetensors and clip_l.safetensors go in: ComfyUI/models/clip/\n",
    "\n",
    "    ae.safetensors goes in: ComfyUI/models/vae/\n",
    "\n",
    "\n",
    "    Tip: You can set the weight_dtype above to one of the fp8 types if you have memory issues.'''\n",
    "\n",
    "    noise = RandomNoise(958831004022715)\n",
    "\n",
    "    width = 1024\n",
    "    height = 1024\n",
    "\n",
    "    model = UNETLoader('flux1-dev.safetensors', 'default')\n",
    "    # The reference sampling implementation auto adjusts the shift value based on the resolution, if you don't want this you can just comment out (Ctrl+/) this ModelSamplingFlux node.\n",
    "    model = ModelSamplingFlux(model, 1.15, 0.5, width, height)\n",
    "\n",
    "    clip = DualCLIPLoader('t5xxl_fp16.safetensors', 'clip_l.safetensors', 'flux')\n",
    "    positive_conditioning = CLIPTextEncode('cute anime girl with massive fluffy fennec ears', clip)\n",
    "    positive_conditioning = FluxGuidance(positive_conditioning, 3.5)\n",
    "\n",
    "    style_model = StyleModelLoader('flux1-redux-dev.safetensors')\n",
    "    clip_vision = CLIPVisionLoader('sigclip_vision_patch14_384.safetensors')\n",
    "    # You can chain multiple `StyleModelApply` nodes if you want to mix multiple images together.\n",
    "    image, _ = LoadImage('sd3_controlnet_example.png')\n",
    "    clip_vision_output = CLIPVisionEncode(clip_vision, image)\n",
    "    positive_conditioning = StyleModelApply(positive_conditioning, style_model, clip_vision_output)\n",
    "\n",
    "    guider = BasicGuider(model, positive_conditioning)\n",
    "    sampler = KSamplerSelect('euler')\n",
    "    sigmas = BasicScheduler(model, 'simple', 20, 1)\n",
    "    latent = EmptySD3LatentImage(width, height, 1)\n",
    "    latent, _ = SamplerCustomAdvanced(noise, guider, sampler, sigmas, latent)\n",
    "    vae = VAELoader('ae.safetensors')\n",
    "    image = VAEDecode(latent, vae)\n",
    "    SaveImage(image, 'ComfyUI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
